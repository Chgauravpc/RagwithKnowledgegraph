# -*- coding: utf-8 -*-
"""KnowledgegraphwithRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dqXQFbEwoKlCDEnZeSC3_WmL7bOpvtwT
"""

pip install --upgrade --quiet langchain langchain-community langchain-groq langchain-experimental

pip install --upgrade --quiet tiktoken yfiles_jupyter_graphs

NEO4J_URI=""
NEO4J_USERNAME=""
NEO4J_PASSWORD=""

groq_api_key=""

pip install --upgrade langchain-neo4j neo4j

from langchain_community.graphs import Neo4jGraph
graph = Neo4jGraph(
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD,
)

pip install --upgrade wikipedia

from langchain.document_loaders import WikipediaLoader

raw_documents = WikipediaLoader(query="Elizabeth I").load()

raw_documents

from langchain.text_splitter import TokenTextSplitter
text_splitter=TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents=text_splitter.split_documents(raw_documents[:3])

pip install --upgrade --quiet langchain_groq

from langchain_groq import ChatGroq

llm=ChatGroq(groq_api_key=groq_api_key,model_name="Gemma2-9b-It")

pip install --upgrade --quiet langchain_experimental

from langchain_experimental.graph_transformers import LLMGraphTransformer
llm_transformer=LLMGraphTransformer(llm=llm)

graph_documents=llm_transformer.convert_to_graph_documents(documents)

graph.add_graph_documents(
    graph_documents,
    baseEntityLabel=True,
    include_source=True
)

default_cypher="MATCH (s) -[r:MENTIONS]->(t) RETURN s,r,t LIMIT 50"

try:
  import google.colab
  from google.colab import output
  output.enable_custom_widget_manager()
except:
  pass

from neo4j import GraphDatabase
from yfiles_jupyter_graphs import GraphWidget
def showGraph(cypher: str = default_cypher):
    driver=GraphDatabase.driver(
        uri=NEO4J_URI,
        auth=(NEO4J_USERNAME,NEO4J_PASSWORD))
    session=driver.session()
    widget= GraphWidget(graph = session.run(cypher).graph())
    widget.node_label_mapping='id'
    display(widget)
    return widget

showGraph()

!pip install langchain langchain-community sentence-transformers neo4j

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Neo4jVector
from langchain_community.graphs import Neo4jGraph
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.prompt import PromptTemplate

!pip install -U langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

NEO4J_URI="neo4j+s://dd69d9f9.databases.neo4j.io"
vector_index = Neo4jVector.from_existing_graph(
     embedding=embeddings,
    url="",
    username="neo4j",
    password="",
    database="neo4j",
    search_type="hybrid",      # or "vector"
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding"
)

graph_query=("CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]")

from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List
class Entities(BaseModel):
     names: List[str]=Field(
         ...,
         description="All the person, organisation or business entities that"
         "appear in the text",
     )

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are extracting organisation and person entities from the text."
        ),
        (
            "human",
            "Use the given format to extract the information from the following input: {question}"
        ),
    ]
)

entity_chain = prompt | llm.with_structured_output(Entities)

entity_chain.invoke({"question" :"where was Amelia Earhart born?"}).names

from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars

def generate_full_text_query(input: str) -> str:
    full_text_query =""
    words = [el for el in remove_lucene_chars(input).split() if el]
    for word in words[:-1]:
        full_text_query += f" {word}~2 AND"
    full_text_query += f" {words[-1]}~2"
    return full_text_query.strip()

def structured_retriever(question: str) -> str:
    result = ""
    entities = entity_chain.invoke({"question": question})

    for entity in entities.names:
        response = graph.query(
            """
            CALL db.index.fulltext.queryNodes('entity', $query, {limit: 2})
            YIELD node, score
            CALL {
                WITH node
                MATCH (node)-[r:!MENTIONS]->(neighbor)
                RETURN node.id + ' ' + type(r) + '->' + neighbor.id AS output
                UNION ALL
                WITH node
                MATCH (node)<-[r:!MENTIONS]-(neighbor)
                RETURN neighbor.id + ' ' + type(r) + '->' + node.id AS output
            }
            RETURN output LIMIT 50
            """,
            {"query": generate_full_text_query(entity)},
        )
        result += "\n".join([el["output"] for el in response])

    return result

graph.query(graph_query)

print(structured_retriever("who was Elizabeth I?"))

def retriver(question: str):
    print(f"Search query: {question}")
    structured_data = structured_retriever (question)
    unstructured_data = [el.page_content for el in vector_index.similarity_search (question)]
    final_data = f"""Structured data:
{structured_data}
Unstructured data:
{"#Document ". join (unstructured_data)}
"""
    return final_data

_template = """Given the following conversation and a follow up question,
rephrase the follow up question to be a standalone question,
in its original language.

Chat History:
{chat_history}

Follow Up Input:
{question}

Standalone question:"""

CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

from typing import List, Tuple
from langchain.schema import HumanMessage, AIMessage
def _format_chat_history(chat_history: List[Tuple[str, str]]):
    buffer = []
    for human, ai in chat_history:
        buffer.append(HumanMessage(content=human))
        buffer.append(AIMessage(content=ai))
    return buffer

from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough

# Fixed branch logic with default
_search_query = RunnableBranch(
    # CASE 1: If input includes chat history
    (
        RunnableLambda(lambda x: bool(x.get("chat_history"))).with_config(
            run_name="HasChatHistoryCheck"
        ),
        (
            RunnablePassthrough.assign(
                chat_history=lambda x: _format_chat_history(x["chat_history"])
            )
            | CONDENSE_QUESTION_PROMPT
            | llm
            | StrOutputParser()
        ),
    ),
    # CASE 2: Else â†’ no chat history, just pass through the question
    RunnableLambda(lambda x: x["question"]),
)

template=""" Answer the question based on the following context:
            {context}
            Question:{question}
            Use the natural language and be concise.
            Answer:"""

prompt=ChatPromptTemplate.from_template(template)

from langchain_core.runnables import RunnableParallel

chain=(
    RunnableParallel(
        {
            "context":_search_query | retriver,
            "question": RunnablePassthrough(),
        }
    )
    | prompt
    | llm
    | StrOutputParser()
)

chain.invoke({"question":"Which house did Elizabeth I belong to?"})